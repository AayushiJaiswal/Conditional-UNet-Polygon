{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPbXetaHShJpjluORpKa7Gk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AayushiJaiswal/Conditional-UNet-Polygon/blob/main/Ayna_ML_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLXppHjzKgT-",
        "outputId": "6b0048f1-b4b1-4f16-8ed8-2fdf99e92be1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Working directory: /content/drive/MyDrive/ayna_assignment\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create folder for assignment\n",
        "import os\n",
        "os.makedirs('/content/drive/MyDrive/ayna_assignment', exist_ok=True)\n",
        "os.chdir('/content/drive/MyDrive/ayna_assignment')\n",
        "print(\"Working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Install packages\n",
        "!pip install wandb torch torchvision torchaudio\n",
        "!pip install pillow matplotlib opencv-python\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import json\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UXdpnyILXoE",
        "outputId": "dbf745e1-16e8-44f4-8839-254abcd4c968"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Download and extract dataset\n",
        "import gdown\n",
        "\n",
        "# Download from Google Drive\n",
        "gdown.download('https://drive.google.com/uc?id=1QXLgo3ZfQPorGwhYVmZUEWO_sU3i1pHM', 'dataset.zip', quiet=False)\n",
        "\n",
        "# Extract\n",
        "!unzip -q dataset.zip\n",
        "!ls -la\n",
        "\n",
        "# Check dataset structure\n",
        "!find dataset -type f | head -10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96_Qgnd4MDWG",
        "outputId": "f00fcb96-a352-4fff-99ea-0db4b619ff26"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QXLgo3ZfQPorGwhYVmZUEWO_sU3i1pHM\n",
            "To: /content/drive/MyDrive/ayna_assignment/dataset.zip\n",
            "100%|██████████| 57.8k/57.8k [00:00<00:00, 74.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 65\n",
            "drwx------ 4 root root  4096 Jul 28 15:13 dataset\n",
            "-rw------- 1 root root 57817 Jul 28 15:19 dataset.zip\n",
            "drwx------ 3 root root  4096 Aug  3 09:13 __MACOSX\n",
            "dataset/.DS_Store\n",
            "dataset/training/data.json\n",
            "dataset/training/outputs/green_octagon.png\n",
            "dataset/training/outputs/blue_hexagon.png\n",
            "dataset/training/outputs/green_diamond.png\n",
            "dataset/training/outputs/cyan_triangle.png\n",
            "dataset/training/outputs/purple_square.png\n",
            "dataset/training/outputs/orange_pentagon.png\n",
            "dataset/training/outputs/magenta_circle.png\n",
            "dataset/training/outputs/blue_triangle.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Explore dataset\n",
        "import json\n",
        "\n",
        "# Load training data\n",
        "with open('dataset/training/data.json', 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(\"Sample data:\", train_data[0])\n",
        "\n",
        "# Check unique colors\n",
        "colors = set()\n",
        "for item in train_data:\n",
        "    colors.add(item['colour'])\n",
        "print(f\"Unique colors: {list(colors)}\")\n",
        "print(f\"Total colors: {len(colors)}\")\n",
        "\n",
        "# Load validation data\n",
        "with open('dataset/validation/data.json', 'r') as f:\n",
        "    val_data = json.load(f)\n",
        "print(f\"Validation samples: {len(val_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phKhtAbBNgqK",
        "outputId": "8e56243a-1e84-460f-b5da-594f6d3a2b62"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 56\n",
            "Sample data: {'input_polygon': 'octagon.png', 'colour': 'cyan', 'output_image': 'cyan_octagon.png'}\n",
            "Unique colors: ['purple', 'green', 'blue', 'magenta', 'red', 'cyan', 'yellow', 'orange']\n",
            "Total colors: 8\n",
            "Validation samples: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Fixed Dataset Class\n",
        "class PolygonDataset(Dataset):\n",
        "    def __init__(self, data_dir, json_file, transform=None):\n",
        "        with open(json_file, 'r') as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Create color to index mapping\n",
        "        colors = set()\n",
        "        for item in self.data:\n",
        "            colors.add(item['colour'])  # Fixed key name\n",
        "        self.color_to_idx = {color: idx for idx, color in enumerate(sorted(colors))}\n",
        "        self.idx_to_color = {idx: color for color, idx in self.color_to_idx.items()}\n",
        "\n",
        "        print(f\"Color mapping: {self.color_to_idx}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Load images - Fixed key names\n",
        "        input_path = os.path.join(self.data_dir, 'inputs', item['input_polygon'])\n",
        "        output_path = os.path.join(self.data_dir, 'outputs', item['output_image'])\n",
        "\n",
        "        input_img = Image.open(input_path).convert('RGB')\n",
        "        output_img = Image.open(output_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            input_img = self.transform(input_img)\n",
        "            output_img = self.transform(output_img)\n",
        "\n",
        "        color_idx = torch.tensor(self.color_to_idx[item['colour']], dtype=torch.long)  # Fixed key\n",
        "\n",
        "        return input_img, color_idx, output_img\n",
        "\n",
        "# Re-create datasets with fixed class\n",
        "train_dataset = PolygonDataset('dataset/training', 'dataset/training/data.json', transform)\n",
        "val_dataset = PolygonDataset('dataset/validation', 'dataset/validation/data.json', transform)\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBkysvBDOHNG",
        "outputId": "9a9cdeda-f4be-48ca-b64d-6dd3e5e3f272"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Color mapping: {'blue': 0, 'cyan': 1, 'green': 2, 'magenta': 3, 'orange': 4, 'purple': 5, 'red': 6, 'yellow': 7}\n",
            "Color mapping: {'blue': 0, 'cyan': 1, 'green': 2, 'yellow': 3}\n",
            "Training dataset size: 56\n",
            "Validation dataset size: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Visualize samples\n",
        "def visualize_samples(dataset, num_samples=4):\n",
        "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        input_img, color_idx, output_img = dataset[i]\n",
        "        color_name = dataset.idx_to_color[color_idx.item()]\n",
        "\n",
        "        # Convert tensors to numpy for plotting\n",
        "        input_np = input_img.permute(1, 2, 0).numpy()\n",
        "        output_np = output_img.permute(1, 2, 0).numpy()\n",
        "\n",
        "        axes[0, i].imshow(input_np)\n",
        "        axes[0, i].set_title(f'Input')\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        axes[1, i].imshow(output_np)\n",
        "        axes[1, i].set_title(f'Output: {color_name}')\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_samples(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "_3vWs41QPGrY",
        "outputId": "5a7465d3-06c9-44ca-dafe-58036fb8f2a8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 8 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABUwAAAJOCAYAAABsl01LAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAO21JREFUeJzt3XuU3GV5B/BnNsluQsIlQhA2YrKE3XAJVqGKRUKsF8CCCEhBKgQ0kOWiWC3iUY+SYBUtCqIUTVBQwHqhIuKlpVqupRZqtZCCZBfZhLpLDyAQ0AhLkrd/+JthZ/Z+ndvncw6nzezszLuLvj75vs/7TC6llAIAAAAAgGgo9wIAAAAAACqFwBQAAAAAICMwBQAAAADICEwBAAAAADICUwAAAACAjMAUAAAAACAjMAUAAAAAyAhMAQAAAAAyAlMAAAAAgIzAFAAAAAAgIzCtUV/72tcil8vFz3/+83IvJTZv3hyrVq2K2267rdxLASg7+zNA5bJHA1Qm+zNTTWDKpNu8eXOsXr3aZgJQYezPAJXLHg1QmezP9UFgCgAAAACQEZjWidNOOy3mzJkT3d3dccwxx8ScOXNi3rx5cd5558XWrVsLz9uwYUPkcrn47Gc/G5deemksWLAgZs2aFcuWLYv/+Z//KXrN17/+9fH6179+wPdauHBh4fXmzZsXERGrV6+OXC4XuVwuVq1aNVk/KkBVsT8DVC57NEBlsj8z2aaXewFMna1bt8bhhx8eBx10UHz2s5+Nn/70p/G5z30uFi1aFGeddVbRc6+55pp49tln45xzzonnnnsuLrvssnjDG94Q69ati5e+9KUjfs958+bFl770pTjrrLPi2GOPjeOOOy4iIl7xildM6M8GUM3szwCVyx4NUJnsz0wmgWkdee655+LEE0+Mj33sYxERceaZZ8YBBxwQX/3qV/ttJg899FB0dnbG/PnzIyLiiCOOiIMOOig+85nPxCWXXDLi95w9e3Ycf/zxcdZZZ8UrXvGKOPnkkyfuBwKoEfZngMpljwaoTPZnJpMr+XXmzDPPLPrz0qVL4+GHH+73vGOOOaawkUREvOY1r4mDDjoofvzjH0/6GgHqkf0ZoHLZowEqk/2ZySIwrSMzZ84szNrImzt3bjz11FP9ntva2trvsba2ttiwYcNkLQ+gbtmfASqXPRqgMtmfmUwC0zoybdq0CX29XC434ON9BywDMDz7M0DlskcDVCb7M5NJYMqAOjs7+z3W0dFR+GS4iD+e3Dz99NP9nrdx48aiPw+26QAwevZngMpljwaoTPZnRktgyoBuvPHG6O7uLvz5nnvuibvvvjve8pa3FB5btGhRPPjgg/H4448XHrv33nvjrrvuKnqt7bbbLiJiwI0HgNGxPwNULns0QGWyPzNa08u9ACrTXnvtFYccckicddZZ8fzzz8fnP//52HnnneP8888vPOfd7353XHLJJXH44YfHihUr4rHHHosvf/nLsd9++8UzzzxTeN6sWbNi3333jW9/+9vR1tYWL3nJS2LJkiWxZMmScvxoAFXN/gxQuezRAJXJ/sxo6TBlQMuXL4/3vve9cfnll8cnP/nJ2G+//eKWW26J3XffvfCcffbZJ6655prYtGlTfOADH4ibbroprr322jjggAP6vd5XvvKVmD9/frz//e+Pk046Kf7xH/9xKn8cgJphfwaoXPZogMpkf2a0cimlVO5FUDk2bNgQLS0tcfHFF8d5551X7uUAkLE/A1QuezRAZbI/M1Y6TAEAAAAAMgJTAAAAAICMwBQAAAAAIGOGKQAAAABARocpAAAAAEBGYAoAAAAAkBGYAgAAAABkBKYAAAAAABmBKQAAAABARmAKAAAAAJARmAIAAAAAZASmAAAAAAAZgSkAAAAAQEZgCgAAAACQEZgCAAAAAGQEpgAAAAAAGYEpAAAAAEBGYAoAAAAAkBGYAgAAAABkBKYAAAAAABmBKQAAAABARmAKAAAAAJARmAIAAAAAZASmAAAAAAAZgSkAAAAAQEZgCgAAAACQEZgCAAAAAGQEpgAAAAAAGYEpAAAAAEBGYAoAAAAAkBGYAgAAAABkBKYAAAAAABmBKQAAAABARmAKAAAAAJARmAIAAAAAZASmAAAAAAAZgSkAAAAAQEZgCgAAAACQEZgCAAAAAGQEpgAAAAAAGYEpAAAAAEBGYAoAAAAAkBGYAgAAAABkBKYAAAAAABmBKQAAAABARmAKAAAAAJARmAIAAAAAZASmAAAAAAAZgSkAAAAAQEZgCgAAAACQEZgCAAAAAGQEpgAAAAAAGYEpAAAAAEBmerkXQOXq7e2N7u7uiIiYP39+NDY2lnlFAAAAADC5dJgyqO7u7jjjjDOivb09enp6yr0cAAAAAJh0OkwpdJL29vYWPd7V1RWdnZ3R0NAQHR0d8fzzzxd9vbGxUecpAAAAADUll1JK5V4E5dXV1RVnnHFGbNy4sejx3t7eePTRRyMiorm5OWbMmFH09ZaWlli7dm0sXLhwqpYKAAAAAJNKh2kdKu0ozXeSPvroo0UdozNnzoyWlpZBv3/Lli1Fnac6TgEAAACodjpM61BpR2m+k3T+/Plx5ZVXxoIFC4b9/pUrV0ZPT09R56mOUwAAAACqnQ7TGjbcbNJ8SJrvJG1paYnW1tZhA9Pp06dHW1tbNDU1Fb1Pacdpns5TAAAAAKqFDtMaNtxs0tKO0sbGxth9992HDTZ7e3ujp6cnXnjhhcL7DNRxmqfzFAAAAIBqocO0hox2NulIO0pLNTY2FoWfpR2npevReQoAAABAtdBhWkNGO5t0pB2lwyntOO27Hp2nAAAAAFQTHaZVaLJmk45Vacdp3mg7T3WcAgAwnMFqYcZGDQ4A/ekwrUKTNZt0oo2281THKQAAwxmsFmZs1OAA0J8O0yowVbNJJ9pIO0/NOgUAYDj5mnH9+vXR2dkZjzzySLmXVBPyNXhKSc0NABkdplWgXLNJJ0tp56lZpwAADCdfE+cbB0pvMTE2M2bMiObm5mhra1NzA0BGh2kFqbTZpJOltPPUrFMAAIbT29sbGzduLHSWzpgxQ004Dvla+4UXXoiNGzdGU1OTEBoAMgLTCtLd3T2m2aTVbv78+bF27dpBZ512d3fHypUrzToFAKBgsNtWjEy+1jYLFgD6E5iWUbXOJp1oZp0CADCYfA3Y1dUVvb29hc7SxYsX12RtPFXytXYulyv8naSrqyumT5+upgag7plhWka1Npt0opl1CgBA6ezSfK3c2tpaV7XxRMvX2h0dHUU1tlmmAKDDdErUy2zSiTbeWad9X8cpOQBAdSqdXdrY2BgLFiyo+1p5vPK19vPPPx8zZswwyxQA+hCYToF6nU060UY76zRP5ykAAAAAIyUwnQRmk06Okc46zRus81THKQBA5RtsdmlLS4s6bgI1NjZGS0tLbNmyxSxTAMiYYToJzCadWqWzTvMGm3mq4xQAoPKZXTo1zDIFgP50mI6D2aSVYaSdp2adAgBUD7NLp4ZZpgDQn8B0HMwmrWylM0/NOgUAAABgOALTccifej/00EMREYW5SjpKK0Np5+lgHaf5QLWhocEpOgBAmZldWh5mmQLAiwSmE0hHaWUbrOO0tEMYAIDyyd/iGmx2KZMjXyvnZ5nmb2WZZQpAPRKYjsFgp96LFy/WUVrBBus4zeVyTtEBACqE2aXlka+VU0pFNXIul4uOjo5IKamRAagbuZRSKvciqo1P7KwNPhEUAKDyrF+/Po466qjC2Ku2trb44Q9/GK2trWVeWX1QIwOADtMxcepdG3wiKABA5TC7tDKokQFAYAoAAFQAs0sBgEohMB0Fp961ySeCAgCUn1tclUWNDEA9M8N0FMwurU3mNAEAlJ/ZpZVFjQxAPdNhOgpOvWuTOU0AAOXjFldlUiMDUM8EpgAAQNmYXQoAVBqBKQAAMOXynaXr168vCksXL14cra2tbnFVCLNMAahHAlMAAGDK6SytDvPnz4+1a9cWZpl2d3fHypUrzTIFoKYJTEfAXKX64PQcAGDq+HyA6mCWKQD1SGA6Ak6/64PTcwAAAAAEpiPg9Ls+OD0HAJh8bm9VJ7exAKgnAlMAAGDKuL1VndzGAqCeCEyhhNNzoK98J1Rvb2+5l1LVGhsb7aFARLi9Va3yt7FSStHW1ha5XC66u7sjl8tFR0dHpJTs80CBGnpiqKHLR2AKJZyeA33lO6E2btxY7qVUtZaWFnsoQA1QKwMjoYaeGGro8hGYDsF8pfpklinQV74T6qGHHir3UqpaQ0ODPRTqnNq6NqiVgZFQQ08MNXT5CEyHYL4SAABMDLU1AFAtBKZDMF+pvpllCgwk3xFlDxhavpPMiTiQp7auLWplYDTU0COjhq4cAlMYhPlMwEDyHVH+gj+0rq6uWLlypblVADVKrQyMhhp6ZNTQlUNgOgDzlYgwnwkYWL4jqrW1tdxLqWjbtm2LGTNmlHsZQAVQW9cmtTIwGmrokVFDVw6B6QDMVwIAgImhtgYAqo3AdADmKwEAwMRQW9c2s0wBqEUN5V4AAAAA1Sk/y3TNmjXR3NxcmGXa3t4ePT095V4eAIyJDlMYhlNzAIDRM7u0PphlCkAtEpjCMHwCKADA6JldCgBUK4FpH07BGYhTcwCA0TO7tL64lQVALRGY9uEUHAAAYPTcygKglghM+3AKzlCcmgMADM+trfqUv5WVUoq2trbI5XLR3d0duVwuOjo6IqWkZgagaghMYYScmgMADM+trfqmZgagFghMYYTMMgUAGJ5bW/VNzQxALWgo9wIAAAAAACqFDtMwZwkAAMZLTU1f5v8DUM0EpmHOEgAAjJeamr7MMgWgmglMw5wlRsdpOQDAi/KdpevXry8KSxcvXhytra1q6jpllikA1UxgCqPktBwA4EU6SwGAWlPXgak5S4yF03IAgBe5rcVQ3M4CoBrVdWDqNBwAAGDyuJ0FQDWq68DUaTjj4bQcAKhnbmsxEm5nAVCN6jowhfFwWg4A1DO3tQCAWiUwhTFyWg4A1DO3tRgNt7MAqCYN5V4AAAAAtS1/O2vNmjXR3NxcuJ3V3t4ePT095V4eABSpyw5T85aYSE7LAYB6opZmLPK3s1JK0dbWFrlcLrq7uyOXy0VHR0eklNTOAFSMugxMzVtiIpllCgDUE7U046F2BqAa1GVgat4SE8ksUwCgnqilGQ+1MwDVwAxTAAAAAIBMXXaYAgAAo2N2KRPJ5wAAUMkEpgAAwLDMLmUimWUKQCWrq8DUqTiTySk5AFDLzC5lIpllCkAlq6vA1Kk4k8kpOQAAAED1q6vA1Kk4k8kpOQBQi9zSYjK5pQVAJaqrwBQAABgdt7SYTG5pAVCJ6iIwdSrOVHJKDgDUEre0mExuaQFQieoiMHUqzlRySg4AAABQveoiMHUqzlRySg4A1ILSW1owldzSAqCc6iIwBQAARqf0lhZMJbe0ACinhnIvAAAAqEwNDQ3R0OCvDJRHLpeLhoaGyOVy5V4KAHVGhykAANBP6Vz2jRs3lntJ1BGfOwFAOdVFYOpTy5lKpfO+ZsyYEfPnz4+Wlhb/WQMAqkbpXPY8tTQTabBZuT53AoByqovA1KeWM5VK5305HQcAaolamolkVi4AlaguAlOfWs5U6u3tjY0bN8YjjzwSEU7HAYDqVnpbSy3NRCqtnd3OAqAS1EVgCgAAjI1Zpkwlt7MAqAR1FZiaZcpkMrsUAKhFZpkyGcwuBaCS1VVgapYpk8nsUgCgnqilGQ+zSwGoZHUVmJplymQyuxQAqGWDzTLN5XLR0dERKSWdpgwr31m6fv366OzsNLsUgIpUV4EpAAAwNoPNMtVpymgM1lnqdhYAlURgCuNkdikAUA/yt7VSStHW1ha5XK6o09StLUai9FZWvnZevHhxtLa2up0FQEUQmMI4mV0KANSTwTpNYSzUzgBUoroMTEvnL/mET8bD7FIAoJ6Ufi5AnpqaoZTeyspTOwNQiRrKvYByyJ+Kr1mzJpqbmwtzl9rb26Onp6fcywMAgKqjpmYo+VtZ7e3tRbNLAaAS1W2Had9TcXOXAABgdEpvbampGcpgs0vN/QegEtVlYAoAAIyPWaaMh9mlAFSyug5MzTJlPErnMDklBwDqiVmmjITZpQBUo7oOTEtPxfNzl9ra2mLt2rWxcOHCci+RCpafw9TZ2RmPPvqoU3IAgAg1NUVKa2YAqAZ1HZiaZcp4lM5hckoOANQjs0wZitmlAFSjug5MAQCA8THLlNFwKwuAaiAwhVEyuxQA4EVmmTIQs0sBqGYCUxgls0sBAIZnlml9M7sUgGomMI3+c5echjMUs0sBAPobbJZpLpeLjo6OSCmpretAvrN0/fr10dnZaXYpAFWpodwLqAT5uUtr1qyJ5ubmwml4e3t79PT0lHt5AABQ8Upr6jy1dX3Jd5a2t7cXdZbmb2WtWbPGrSwAKp4O0+g/d8knezIQs0sBAAaXr6lTStHW1ha5XK6o01RtXR9Kb2Pla+bFixdHa2urW1kAVAWBKYyQ2aUAAMPLd5p2dHTEypUrY+PGjeVeEmWkZgagGglM+yiduwR9mV0KADC80ttbeT4noLaV3sbKUzMDUI0Epn2UnoYDAAATIz/LtK2tLdauXRsLFy4s95KYQKW3sQCgmglM+yg9DXcKDgAAY1N6e8ss09o22OxS8/4BqEYN5V5AJfOJngAAMDb521tr1qyJ5ubmci+HKZafXbpmzRqzSwGoOjpMh+AUnIj+85iclgMADM8s0/pgdikAtUhgCsMoncfkkz4BAMbOLNPaYnYpALVIYDqA0nlLTsHrW+k8JqflAAAjN9gs01wuFx0dHZFSUmNXoXxn6fr166Ozs9PsUgBqihmmAyidt2SWKQAAjM1gs0zV2NUt31na3t5e1FlqdikAtUCH6QBK5y2ZZVqfzC4FABi/fG2dUoq2trbI5XJFnaZq7OpUegsrXysvXrw4Wltb3cYCoKoJTGEQZpcCAEycfKdpR0dHrFy5MjZu3FjuJTGB1MoA1BKBKQzC7FJgIPm51tu2bSv3Uipa6aclA5Te4srzeQHVpfQWVp5aGRiKGnpk1NCVQ2AKAKOQn7nX9y/79Nfb2+vTkoERye+rbW1tsXbt2li4cGG5l8QQSm9hAYyEGnpk1NCVQ2A6hNJP9HT6XR/MLgWGkp+5B8DYlNbYZplWl8Fml6qVgaGooak2AtMhlM5ZcvpdH8wuBQCYPGaZ1ha1MgC1SGA6hNI5S06/64PZpUBf+U6ohoaGci+lquk8AvLMMq1OZpcCo6GGnhhq6PIRmALAEPKdUA7KxqexsVHnETAkt7kqm9mlwGiooSeGGrp8BKYjYJYpQP3Kd0IBMLEGm2Way+Wio6MjUkpq7QqQ7yxdv359dHZ2ml0KjIgammqXSymlci+i0vX29kZPT09hzlJPT080Nzc7/a5R69evj6OOOioeeuihiIhoa2uLH/7wh9Ha2lrmlQEA1I7SGjs/y3TGjBlq7QrS1dVV1Fma7xZbuHBh0exSoSkAtUSH6QiYZVofSucyOTUHAJg8+Ro7pRRtbW2Ry+WKOk3V2pWhdL5/vkZevHhxtLa2ml0KQE0SmEKmdC6TT/wEAJh8+Tl3pZ2mVCY1MgD1QGAKmdLTc5/4CQAw+Upvc+X53IDyKr19ladGBqAeCEwBAICK093dHStXrjTLtExKb18BQD0RmI5C6Sd5OvWuDWaXAgCUX2mtbZZpeQ02u1SNDEA9EJiOQul8JafetcHsUgCA8jPLtLKpkQGoJwLTUSidr+TUuzaYXQoAUH5mmVYGs0sBQGAKAABUMLe6ppbZpQAgMB0Ts0xrg9mlAACVxyzT8jK7FAAEpmNilmltMLsUAKDymGVaWdTIANQjgekYDDbLNJfLRUdHR6SUdJpWsHxn6fr166Ozs9PsUgCACmKWaXmYXQoALxKYTiCdptXBXCYAgOqj1p5camQAeJHAdBzy85UaGhoi4sVT2Xyn6fPPP9/v+U7Dp07+30ffE/KIiK6urqJr+Pl/H+YyAQBUjsFmmbrVNbEGu31ldikA9SyXUkrlXkS16u3tjZ6ensLw+a6urli5cmX09PREc3Nz0RWiiD8Gck7Dp05XV1ecccYZ/eZe9fb2Fs0szV8vamxsjN13311BCABQAfK1duks0xkzZkRzc7NO0wmSr5nzDQX5v9ssXLiwaHapGhmAeqLDdBzy85Xypk+fHm1tbdHU1FT0vPyp7ZYtW4o6T3WcTqzSjtLBOklnzpwZLS0t0dLSEq2treYxAQBUoHytnVKKtra2yOVyRZ2mTU1NhXCPsevt7Y2NGzf26yxdvHixWhmAuiUwnUD5T/QsLdzynaf5uUv5zlMdpxMrP3cp330wWCdpXr6jFACAypWvsUs7TZkc+do531kKAPVIYDqBSjtO80o7TwfrOO37OjpPhzdcR6lOUgCA6pevsZ9//vmikVe9vb3R1dUV06dPVzuPQb6W7urqKpr539jYGAsWLFA7A1DXBKZToLTzdLCO0zydpyMz0o5SnaQAALUnX0ubZTo2+Vo633AAALxIYDoFzDodn5F+2r2OUgCA2tXY2BgtLS2xZcsWs0wnwGCzS1taWur27x0AkCcwLQOzTkentJM0T0cpAED9MMt0cpldCgAvEpiWwXhnndZ6x6lPuwcAoNRws0y3bdtWxtVVH7NLAWBwAtMKMtJZp7XecerT7gEAGKnBPheAoeVrbACgP4FpBRnprNMtW7bEhg0bCh2nfb+/GjtPfdo9AAAjNdgsU8bG7FIA6C+XUkrlXgQD6+3tjZ6enn6zTru7u+OCCy6Inp6eosertfO0q6trVJ92r5ADAKhf+RrZLNOJsXDhwqLZpWptANBhWtGGmnXa1NQUDQ0NEVE9s0592j0AAOOVr5FTSgPexmJ01NwA0J8O0ypU2nman3Xa09MTzc3NFTvrtLSTNE9HKQAAozXYbSxGR80NAP3pMK1Cw806HazjtO/3T0XnqU+7BwBgsgx2GwsAYLx0mNaAkXac5k1V5+lIZ5PmOd0GAAAAoNx0mNaA4TpO8yZ71qlPuwcAAACg2ukwrUGDzXOa7FmnPu0eAAAAgGqnw7QGDTbPaaJmnfq0ewAAAABqlQ7TOjJRs0592j0AAAAAtUqHaR0Z76zTPJ92DwAAAECt0mFax0Y767Tv9/m0ewAAAABqkQ7TOjbaWacRYTYpAAAAADVNhyn9DDbrNJfLmU0KAAAAQE3TYUo/g806bWho0FEKAAAAQE3TYcqw8h2nuVxORykAAAAANU1gCgAAAACQaSj3AgAAAAAAKoXAFAAAAAAgIzAFAAAAAMgITAEAAAAAMgJTAAAAAICMwBQAAAAAICMwBQAAAADICEwBAAAAADICUwAAAACAjMAUAAAAACAjMAUAAAAAyAhMAQAAAAAyAlMAAAAAgIzAFAAAAAAgIzAFAAAAAMgITAEAAAAAMgJTAAAAAICMwBQAAAAAICMwBQAAAADICEwBAAAAADICUwAAAACAjMAUAAAAACAjMAUAAAAAyAhMAQAAAAAyAlMAAAAAgIzAFAAAAAAgIzAFAAAAAMgITAEAAAAAMgJTAAAAAICMwBQAAAAAICMwBQAAAADICEwBAAAAADICUwAAAACAjMAUAAAAACAjMAUAAAAAyAhMAQAAAAAyAlMAAAAAgIzAFAAAAAAgIzAFAAAAAMgITAEAAAAAMgJTAAAAAICMwBQAAAAAICMwBQAAAADICEwBAAAAADICUwAAAACAjMAUAAAAACAjMAUAAAAAyAhMAQAAAAAyAlMAAAAAgIzAFAAAAAAgIzAFAAAAAMgITAEAAAAAMgJTAAAAAICMwBQAAAAAICMwBQAAAADICEwBAAAAADICUwAAAACAjMAUAAAAACAjMAUAAAAAyAhMAYCyWLVqVeRyuXIvAwAAoIjAdBLdf//9cfLJJ8f8+fOjqakpmpub453vfGfcf//943rdT33qU3HjjTdOzCKH8e///u+xatWqePrpp6fk/QAmij0YYGrZd+vLAw88EKtWrYoNGzaUeynACNij64s9evwEppPkhhtuiAMOOCD+9V//Nd71rnfFFVdcEStWrIhbb701DjjggPje97435tee6g1p9erVNiSgqtiDAaaWfbf+PPDAA7F69Wp/GYcqYI+uP/bo8Zte7gXUol//+tdxyimnxJ577hl33HFHzJs3r/C1973vfbF06dI45ZRT4r777os999yzjCsFqD324PLYtm1b9Pb2xsyZM8u9FGCK2XcBKpc9GsYoMeHa29tTRKQ77rhjwK/ffvvtKSJSe3t74bFTTz01LViwoN9zL7jggtT3X1NE9Pvn1FNPLXrur371q/SXf/mXafvtt08veclL0rnnnpv+8Ic/FF6jq6srRUS6+uqr+71fRKQLLrig6PVK/+nq6koppfT444+nX/3qV+n3v//9iH4v1157bXr1q1+dZs2alXbaaae0dOnSdPPNN6eUUlq+fHnaeeedU29vb7/ve/Ob35za2toKf77qqqvSn//5n6d58+alxsbGtM8++6Qrrrii3/ctWLAgHXnkkenOO+9Mr371q1NTU1NqaWlJX//610e0XqA62YP7i4h0zjnnpOuuuy61tbWlpqamdMABB6Tbb7+96Hkj/T2Uvua+++6bpk+fnr73ve8Vfr6LL744XXLJJenlL395mjlzZjr00EPTunXrhn3dlP74vxcHHHBAmjlzZpo7d2468cQT0yOPPDLszwmUh323v/we+Z3vfCfts88+aebMmem1r31tuu+++1JKKX35y19OixYtSk1NTWnZsmWF98i744470vHHH5/22GOP1NjYmF72spelv/7rv06bN2/u917592hqakr77bdfuuGGGwb8/W7dujVdeumlad99901NTU1p1113TStXrkxPPvlk0fNGUkNfffXVA/6ubr311pRSSjfeeGP6i7/4i7T77runxsbGtOeee6YLL7wwbdmyZdjfHTCx7NH92aPt0SOhw3QS/OAHP4iFCxfG0qVLB/z6oYceGgsXLowf/ehHo37ta6+9Nk4//fR4zWteEytXroyIiEWLFhU954QTToiFCxfGRRddFP/xH/8RX/jCF+Kpp56Ka665ZlTvddxxx0VHR0d885vfjEsvvTR22WWXiIjCidTll18eq1evjltvvTVe//rXD/laq1evjlWrVsXBBx8cF154YTQ2Nsbdd98dt9xySxx22GFxyimnxDXXXBM333xzHHXUUYXv+7//+7+45ZZb4oILLig89qUvfSn222+/OProo2P69Onxgx/8IM4+++zYtm1bnHPOOUXv+9BDD8Xxxx8fK1asiFNPPTWuuuqqOO200+LAAw+M/fbbb1S/D6A62IMHdvvtt8e3v/3tOPfcc6OpqSmuuOKKOOKII+Kee+6JJUuWjGptebfcckt85zvfife85z2xyy67xMKFCwtfu+aaa+LZZ5+Nc845J5577rm47LLL4g1veEOsW7cuXvrSlw76mp/85CfjYx/7WJxwwglx+umnx+OPPx5f/OIX49BDD41f/vKXsdNOO41prcDkse8O7M4774ybbrqpUJ9edNFFcdRRR8X5558fV1xxRZx99tnx1FNPxd/93d/Fu9/97rjlllsK33v99dfH5s2b46yzzoqdd9457rnnnvjiF78Yv/nNb+L6668vPO9HP/pRnHjiibH//vvHRRddFE899VSsWLEi5s+f32897e3t8bWvfS3e9a53xbnnnhtdXV1x+eWXxy9/+cu46667YsaMGYXnDldDH3rooXHuuefGF77whfjIRz4S++yzT0RE4f9+7Wtfizlz5sQHPvCBmDNnTtxyyy3x8Y9/PJ555pm4+OKLR/XvBRgfe/TA7NH26GGVO7GtNU8//XSKiPS2t71tyOcdffTRKSLSM888k1IaXVfP7NmzC6c2Az336KOPLnr87LPPThGR7r333pTSyE9wUkrp4osvLjq1Gej98qcUg+ns7EwNDQ3p2GOPTVu3bi362rZt21JKfzxNednLXpZOPPHEoq9fcsklKZfLpYcffrjw2ECnNocffnjac889ix5bsGBBv5O0xx57LDU1NaW/+Zu/GXLNQHWyBw8sslPln//854XHNm7cmGbOnJmOPfbYwmOj7TBtaGhI999/f9Hj+Z9v1qxZ6Te/+U3h8bvvvjtFRHr/+98/6Otu2LAhTZs2LX3yk58ses1169al6dOn93scKD/77sAiIjU1NRW9zpo1a1JEpN12263we0gppQ9/+MP93nOgeveiiy5KuVwubdy4sfDY/vvvn172spelZ599tvDYbbfdliKi6Pd75513pohI3/jGN4pe85//+Z/7PT7SGvr6668f9Pcx0Prb29vTdtttl5577rl+XwMmhz16YPZoe/RI+NCnCfbss89GRMT2228/5PPyX3/mmWcmfA2lXZbvfe97IyLixz/+8YS+z6pVqyKlNOzpzY033hjbtm2Lj3/849HQUPwfuVwuFxERDQ0N8c53vjNuuummwu8wIuIb3/hGHHzwwdHS0lJ4bNasWYX/f9OmTfHEE0/EsmXL4uGHH45NmzYVvf6+++5bdJI2b968WLx4cTz88MOj/nmBymcPHtyf/dmfxYEHHlj488tf/vJ429veFjfffHNs3bp1TGtYtmxZ7LvvvgN+7Zhjjik6PX/Na14TBx100JC/hxtuuCG2bdsWJ5xwQjzxxBOFf3bbbbdobW2NW2+9dUzrBCaPfXdwb3zjG4s67w866KCIiHj7299e9PvKP963Pu1b7/7+97+PJ554Ig4++OBIKcUvf/nLiIjo6emJdevWxfLly2POnDmF5y9btiz233//orVcf/31seOOO8ab3/zmov31wAMPjDlz5vTbX8dbQ/dd/7PPPhtPPPFELF26NDZv3hwPPvjgiF4DGD979ODs0X9kjx6cwHSC5f+L1Tf0G8hIN66xaG1tLfrzokWLoqGhoWyfjvbrX/86GhoaBv1Ldd7y5cvjD3/4Q+ET+tavXx//9V//FaecckrR8+66665405veFLNnz46ddtop5s2bFx/5yEciIvoFpi9/+cv7vc/cuXPjqaeeGs+PBFQoe/DgStcVEdHW1habN2+Oxx9/fEyv2fcwa6TvN9TvobOzM1JK0draGvPmzSv651e/+lU89thjY1onMHnsu4MrrUN33HHHiIjYY489Bny8b336yCOPxGmnnRYveclLYs6cOTFv3rxYtmxZRLxY727cuDEiIvbaa69+7136WGdnZ2zatCl23XXXfvvr7373u37763hr6Pvvvz+OPfbY2HHHHWOHHXaIefPmxcknn1y0fmDy2aMHZ4+2Rw/HDNMJtuOOO8buu+8e991335DPu++++2L+/Pmxww47RMSLnZalxtr101fpa0/me43HvvvuGwceeGBcd911sXz58rjuuuuisbExTjjhhMJzfv3rX8cb3/jG2HvvveOSSy6JPfbYIxobG+PHP/5xXHrppbFt27ai15w2bdqA75VSmtSfBSgPe/D4jHZtfU+nJ8K2bdsil8vFP/3TPw24f/c9nQcqg313cIPVocPVp1u3bo03v/nN8eSTT8aHPvSh2HvvvWP27NnR3d0dp512Wr96dyS2bdsWu+66a3zjG98Y8Ot9PzV7JGscytNPPx3Lli2LHXbYIS688MJYtGhRzJw5M37xi1/Ehz70oTGtHxgbe/Tg7NH26OEITCfBUUcdFVdeeWX827/9WxxyyCH9vn7nnXfGhg0bor29vfDY3Llz4+mnn+733PypRF+DbSh5nZ2dRV0/Dz30UGzbtq3Qbj537tyIiH7vN5b3GolFixbFtm3b4oEHHohXvvKVQz53+fLl8YEPfCAeffTR+Id/+Ic48sgjC+uN+OPA6ueffz5uuummolMV1zSBPHvw4Osq1dHREdttt12hCBvN72Gs79f36lOpRYsWRUopWlpaoq2tbdTvCZSHfXdirVu3Ljo6OuLrX/96LF++vPD4T37yk6LnLViwICL++POWKn1s0aJF8dOf/jRe97rXTdhh12C/q9tuuy1++9vfxg033BCHHnpo4fGurq4JeV9gdOzRE8seXT9cyZ8EH/zgB2PWrFnR3t4ev/3tb4u+9uSTT8aZZ54Z2223XXzwgx8sPL5o0aLYtGlT0cnPo48+Wrie3tfs2bMH3Lzy/v7v/77oz1/84hcjIuItb3lLRETssMMOscsuu8Qdd9xR9LwrrrhiwPeK6L95RUQ88cQT8eCDD8bmzZsHXUvEH+fYNTQ0xIUXXtjvtKL0BOSkk06KXC4X73vf++Lhhx8utIXn5U9S+n7fpk2b4uqrrx5yDUD9sAcP7Gc/+1n84he/KPz5f//3f+P73/9+HHbYYYW9dTS/h+HceOON0d3dXfjzPffcE3fffXfh9zCQ4447LqZNmxarV6/u978PKaV+/z6BymDfnVgD1bsppbjsssuKntfc3BxLliyJa665Jn73u98VHr/99ttj3bp1Rc894YQTYuvWrfGJT3yi3/tt2bJlyN/vYAb7XQ20/t7e3gF/38Dks0dPLHt0/dBhOglaW1vj61//erzzne+M/fffP1asWBEtLS2xYcOG+OpXvxpPPPFEfPOb34xFixYVvucd73hHfOhDH4pjjz02zj333Ni8eXN86Utfira2tqK/4EZEHHjggfHTn/40Lrnkkmhubo6WlpbCIOKIP54MHH300XHEEUfEz372s7juuuvir/7qr+JP/uRPCs85/fTT49Of/nScfvrp8ad/+qdxxx13REdHR7+fJf8BIR/96EfjHe94R8yYMSPe+ta3xuzZs+Pyyy+P1atXx6233jrkYOW99torPvrRj8YnPvGJWLp0aRx33HHR1NQU//mf/xnNzc1x0UUXFZ47b968OOKII+L666+PnXbaKY488sii1zrssMOisbEx3vrWt0Z7e3v87ne/iyuvvDJ23XXXePTRR0f2LwioafbggS1ZsiQOP/zwOPfcc6OpqalQFK1evXpMv4fh7LXXXnHIIYfEWWedFc8//3x8/vOfj5133jnOP//8Qb9n0aJF8bd/+7fx4Q9/ODZs2BDHHHNMbL/99tHV1RXf+973YuXKlXHeeeeNah3A5LPvTqy99947Fi1aFOedd150d3fHDjvsEN/97ncHnE33qU99Kt72trfF6173unjXu94VTz31VFx++eWxZMmSor+gL1u2LNrb2+Oiiy6K//7v/47DDjssZsyYEZ2dnXH99dfHZZddFscff/yo1vnKV74ypk2bFp/5zGdi06ZN0dTUFG94wxvi4IMPjrlz58app54a5557buRyubj22muNxIIysUdPLHt0HUlMmvvuuy+ddNJJaffdd08zZsxIu+22WzrppJPSunXrBnz+v/zLv6QlS5akxsbGtHjx4nTdddelCy64IJX+a3rwwQfToYcemmbNmpUiIp166qkppVR47gMPPJCOP/74tP3226e5c+em97znPekPf/hD0Wts3rw5rVixIu24445p++23TyeccEJ67LHHUkSkCy64oOi5n/jEJ9L8+fNTQ0NDiojU1dVV9H633nrriH4fV111VXrVq16Vmpqa0ty5c9OyZcvST37yk37P+853vpMiIq1cuXLA17npppvSK17xijRz5sy0cOHC9JnPfCZdddVVRWtLKaUFCxakI488st/3L1u2LC1btmxEawaqlz34RRGRzjnnnHTdddel1tbW1NTUlF71qlcN+L0j/T3kX7NUV1dXioh08cUXp8997nNpjz32SE1NTWnp0qXp3nvvLXruQK+bUkrf/e530yGHHJJmz56dZs+enfbee+90zjnnpPXr1w/7swLlY9990UB7ZN/9sa9bb701RUS6/vrrC4898MAD6U1velOaM2dO2mWXXdIZZ5yR7r333hQR6eqrry76/m9961tp7733Tk1NTWnJkiXppptuSm9/+9vT3nvv3W9da9euTQceeGCaNWtW2n777dP++++fzj///NTT01N4zmhq6CuvvDLtueeeadq0aUW/m7vuuiu99rWvTbNmzUrNzc3p/PPPTzfffPOo/u4ATCx79Ivs0fbokcilJEauFatWrYrVq1fH448/Hrvssku5lzNm3//+9+OYY46JO+64I5YuXVru5QCMSCXvwblcLs4555y4/PLLJ/29NmzYEC0tLXHxxRfrBgUmVSXvu+X2yle+MubNm9dvph7AVLFHD84eXR3MMKXiXHnllbHnnnsOOJAaAAD4oxdeeCG2bNlS9Nhtt90W995776ReSQVgePbo6maGKRXjW9/6Vtx3333xox/9KC677LKK+AQ8AACoVN3d3fGmN70pTj755Ghubo4HH3wwvvzlL8duu+0WZ555ZrmXB1DX7NHVTWBKxTjppJNizpw5sWLFijj77LPLvRwAAKhoc+fOjQMPPDC+8pWvxOOPPx6zZ8+OI488Mj796U/HzjvvXO7lAdQ1e3R1M8MUAAAAACBjhikAAAAAQEZgCgAAAACQEZgCAAAAAGQEpgAAAAAAGYEpAAAAAEBGYAoAAAAAkBGYAgAAAABkBKYAAAAAABmBKQAAAABARmAKAAAAAJARmAIAAAAAZASmAAAAAAAZgSkAAAAAQEZgCgAAAACQEZgCAAAAAGQEpgAAAAAAGYEpAAAAAEBGYAoAAAAAkBGYAgAAAABkBKYAAAAAABmBKQAAAABARmAKAAAAAJARmAIAAAAAZASmAAAAAAAZgSkAAAAAQEZgCgAAAACQEZgCAAAAAGQEpgAAAAAAGYEpAAAAAEBGYAoAAAAAkBGYAgAAAABkBKYAAAAAABmBKQAAAABARmAKAAAAAJARmAIAAAAAZASmAAAAAAAZgSkAAAAAQEZgCgAAAACQEZgCAAAAAGQEpgAAAAAAGYEpAAAAAEBGYAoAAAAAkBGYAgAAAABkBKYAAAAAABmBKQAAAABARmAKAAAAAJARmAIAAAAAZASmAAAAAAAZgSkAAAAAQEZgCgAAAACQEZgCAAAAAGQEpgwpl/0DAAAAAPVAYAoAAAAAkJle7gVQGYbrIh3s62miFwIAAAAAZaTDFAAAAAAgo8O0Tk3UXNLS19FxCgAAAEA102EKAAAAAJDRYVrjpvoT7s06BQAAAKCa6TAFAAAAAMjoMK0xU91ROlI6TwEAAACoBjpMAQAAAAAyOkyrVKV2ko5W6c+h4xQAgGHVSjFcKRThAFBEhykAAAAAQEaHaZWol0N0s04BABhUvRTFUy3/e1V0A0BE6DAFAAAAACjQYVphHJoPzKxTAAAAAKaCDlMAAAAAgIwO0zLTUTo2Zp0CANQRRfPUMMsUACJChykAAAAAQIEO0yniUHxq6DwFAAAAYDx0mAIAAAAAZHSYThIdpZWl9N+HjlMAgCqgqC4Ps0wBqHM6TAEAAAAAMjpMx8mhd3Uy6xQAAACAgegwBQAAAADI6DAFAAAqi2tclcEsUwDqlA5TAAAAAICMDtMxcuhdmxyiAwBACUUyAHVGhykAAAAAQEaHKQAAUBlc4wIAKoAOUwAAAACAjA7TUXLoXR+MaQIAgBKKZADqhA5TAAAAAICMDlMAAKC8XOMCACqIDlMAAAAAgIwOUwAAoDx0llYns0wBqHE6TAEAAAAAMjpMR8jhd31yeA4AAABQX3SYAgAAAABkdJgCAABTy/Wt2uA6FgA1SmAKAEPY2rs1nul+Jrb2bi33UqratMZpscP8HWJa47RyLwUAAGBIAlMYAYfnUL+e6X4mfnDGD2LTxk3lXkpV26llp3jr2rfGTgt3KvdSAJhoimVgIG4TTAx7a1kITAFgCFt7t8amjZviyYeeLPdSqlquIRdbX9ClCwAAVD6B6TAciAAAwARRXAMAVaCh3AsAAAAAAKgUAlMYhVxojAAAgAEplgGoEQJTAAAAAICMGaaDcDAKAAATRHENAFQRHaYAAAAAABmBKQAAABPHLFMAqpzAFAAAAAAgY4YpjEH+wDyVdRUAABVOlyEAUIV0mAIAAAAAZHSYlnAIDgAAMAFcywKgSukwBQAAAADI6DCFcXBoDgAwANe26EvRDECV0WEKAAAAAJARmAIAAAAAZASmAAAAAAAZM0wzxiwBAMA4KaoZilmmAFQJHaYAAAAAABkdpjABHJYDAHVNZykAUEN0mAIAAAAAZOq+w9RhOAAAwBRyPQuACqfDFAAAAAAgU/cdpjCRHJYDAHXFdS0AoAbpMAUAAAAAyAhMAQAAmHq50KUMQEUSmAIAAAAAZOp2hqmDTCaTWaYAQE1TTDORFM8AVBgdpgAAAAAAGYEpAAAAAEBGYAoAAAAAkKnbGaYAAMAomV3KZDLLFIAKocMUAAAAACBTdx2mDsWZSg7JAQAAAKqLDlMAAAAAgEzddZgCAACj5JoWU8k1LQDKTIcpAAAAAECmbjpMHYpTTg7JAQAAAKqDDlMAAAAAgEzddJgCAACj5JoW5eSaFgBlosMUAAAAACCjwxQAABhYvrNPpynloLMUgDLRYQoAAAAAkKmbDlOH45STw3EAAACA6qDDFAAAAAAgUzcdpgAAwBi5rsVUcj0LgDLTYQoAAAAAkKm7DlOH40wlh+MAAAAA1UWHKQAAAABApu46TAEAgDFyXYvJ5HoWABVChykAAAAAQEaHKUwCh+MAQE3TacpEUjwDUGF0mAIAAAAAZOo2ME3hIBMAAAAAKFa3gSkAAAAAQCkzTAEAgLExy5TxcOUPgAqlwxQAAAAAIFP3HaYOxZlIDskBAAAAqpsOUwAAAACATN13mAIAAOPk2haj4VoWABVOhykAAAAAQEaHKUwAh+QAAAAAtUGHKQAAAABARodpxtglAAAYJ0U1Q3EtC4AqocMUAAAAACCjwxTGwSE5AMAAdJrSl6IZgCqjwxQAAAAAICMwLZHCASgAAAAA1CuBKQAAAABAxgxTAABgcphlWt9c3QOgSukwBQAAAADI6DCFMXBYDgAAAFCbdJgCAAAAAGR0mA7CuCUAAJggiuv64joWAFVOhykAAAAAQEaHKYyCw3IAgHHQaVrbFMsA1AgdpgAAAAAAGYEpAAAAAEBGYAoAAAAAkDHDdBjGLBFhHBMAwIRSZNcWxTIANUZgCgBDmNY4LXZq2SlyDf5WPx47tewU0xqnlXsZAAAAw8qllJwHjoC/Jtc3/yWB+rW1d2s82/NsbH1ha7mXUtWmNU6L7XffXmgKFFNk1wbFMgA1RmA6Qmq5+ua/JAAAk0CRXRsUywDUGFfyR8iYJQAAmGCK7OomKAWgRjWUewEAAAAAAJVChykMwaE5AMAU0GlaXRTJANQ4HaYAAAAAABmBKQAAAABARmAKAAAAAJAxw3SUjFeqD8YyAQCUgWK7simSAagTOkwBAAAAADICUwAAAACAjMAUAAAAACBjhukYGa9Um4xlAgCoAIrtyqJIBqDO6DAFAAAAAMgITAEAAAAAMgJTAAAAAICMGabjVDrOx5il6mIcEwBABTPLtLwUywDUKR2mAAAAAAAZHaYTbLBDWIfi5eVwHACgiuk0nVqKZwDqnA5TAAAAAICMDtMpYtbp1HIoDgAAAMBY6DAFAAAAAMjoMC0Ts04nlo5SAIA6YJbp5FJUA0BE6DAFAAAAACjQYVphzDodmkNvAAAAACaTDlMAAAAAgIwO0wpX77NOdZQCANCPWaYTS9ENAEV0mAIAAAAAZHSYVqlanXXqcBsAgBFTPAIAk0CHKQAAAABARodpjai2jlPNAAAAAABUIh2mAAAAAAAZHaY1arAOznJ1nuooBQAAAKAa6DAFAAAAAMjoMK0zUzXrVEcpAAAAANVIhykAAAAAQEaHaZ0b76xTnaQAAAAA1BIdpgAAAAAAGR2mDGiwWac6SgEAAACoZTpMAQAAAAAyOkwZEZ2lAAAAANQDHaYAAAAAABmBKQAAAABARmAKAAAAAJARmAIAAAAAZASmAAAAAAAZgSkAAAAAQEZgCgAAAACQEZgCAAAAAGQEpgAAAAAAGYEpAAAAAEBGYAoAAAAAkBGYAgAAAABkBKYAAAAAABmBKQAAAABARmAKAAAAAJARmAIAAAAAZASmAAAAAAAZgSkAAAAAQEZgCgAAAACQEZgCAAAAAGQEpgAAAAAAGYEpAAAAAEBGYAoAAAAAkBGYAgAAAABkBKYAAAAAABmBKQAAAABARmAKAAAAAJARmAIAAAAAZASmAAAAAAAZgSkAAAAAQEZgCgAAAACQEZgCAAAAAGQEpgAAAAAAGYEpAAAAAEBGYAoAAAAAkBGYAgAAAABkBKYAAAAAABmBKQAAAABARmAKAAAAAJARmAIAAAAAZASmAAAAAAAZgSkAAAAAQEZgCgAAAACQEZgCAAAAAGQEpgAAAAAAGYEpAAAAAEBGYAoAAAAAkBGYAgAAAABkBKYAAAAAABmBKQAAAABARmAKAAAAAJARmAIAAAAAZASmAAAAAAAZgSkAAAAAQEZgCgAAAACQ+X+8o0ptgimv6AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7:  UNet\n",
        "class SimpleConditionalUNet(nn.Module):\n",
        "    def __init__(self, n_channels=3, n_classes=3, num_colors=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Conv2d(n_channels, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.enc2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.enc3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Bottleneck with color conditioning\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Color embedding\n",
        "        self.color_embed = nn.Embedding(num_colors, 512)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.dec2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.dec3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.final = nn.Conv2d(64, n_classes, 1)\n",
        "\n",
        "    def forward(self, x, color_idx):\n",
        "        # Encoder\n",
        "        x1 = self.enc1(x)  # 64x64\n",
        "        x2 = self.enc2(x1)  # 32x32\n",
        "        x3 = self.enc3(x2)  # 16x16\n",
        "\n",
        "        # Bottleneck\n",
        "        x4 = self.bottleneck(x3)  # 16x16x512\n",
        "\n",
        "        # Add color conditioning\n",
        "        color_emb = self.color_embed(color_idx)\n",
        "        color_emb = color_emb.view(-1, 512, 1, 1)\n",
        "        color_emb = color_emb.expand(-1, -1, x4.shape[2], x4.shape[3])\n",
        "        x4 = x4 + color_emb\n",
        "\n",
        "        # Decoder (no skip connections)\n",
        "        x = self.dec1(x4)  # 32x32x256\n",
        "        x = self.dec2(x)   # 64x64x128\n",
        "        x = self.dec3(x)   # 128x128x64\n",
        "\n",
        "        return torch.sigmoid(self.final(x))\n",
        "\n",
        "# Create simplified model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SimpleConditionalUNet(num_colors=len(train_dataset.color_to_idx)).to(device)\n",
        "\n",
        "print(f\"Simplified model created on device: {device}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9MfV6vxPOfV",
        "outputId": "3dca7779-7008-4f6a-9ff3-00cbf5469eef"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simplified model created on device: cuda\n",
            "Total parameters: 3,018,435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Training setup\n",
        "# Login to wandb\n",
        "wandb.login()\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(\n",
        "    project=\"ayna-polygon-coloring\",\n",
        "    name=\"conditional-unet-v1\",\n",
        "    config={\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"batch_size\": 16,\n",
        "        \"epochs\": 30,\n",
        "        \"image_size\": 128,\n",
        "        \"architecture\": \"conditional-unet\",\n",
        "        \"num_colors\": len(train_dataset.color_to_idx)\n",
        "    }\n",
        ")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
        "\n",
        "print(\"Training setup complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "Il8DoBW0PRRn",
        "outputId": "c38f6747-b541-4510-f457-e530f24accea"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>█▄▁▁▁▁▁▂▁▁▁▂▁▂▁▂▁▂▂▂▂▁▂▂▂▂▁▂▂▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>███████▄▄▄▄▄▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.09217</td></tr><tr><td>epoch</td><td>29</td></tr><tr><td>learning_rate</td><td>6e-05</td></tr><tr><td>train_loss</td><td>0.10578</td></tr><tr><td>val_loss</td><td>0.06575</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">conditional-unet-v1</strong> at: <a href='https://wandb.ai/jaiswalaayushi198-visvesvaraya-technological-university/ayna-polygon-coloring/runs/cs5sqjpz' target=\"_blank\">https://wandb.ai/jaiswalaayushi198-visvesvaraya-technological-university/ayna-polygon-coloring/runs/cs5sqjpz</a><br> View project at: <a href='https://wandb.ai/jaiswalaayushi198-visvesvaraya-technological-university/ayna-polygon-coloring' target=\"_blank\">https://wandb.ai/jaiswalaayushi198-visvesvaraya-technological-university/ayna-polygon-coloring</a><br>Synced 5 W&B file(s), 6 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250803_093746-cs5sqjpz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/ayna_assignment/wandb/run-20250803_094704-6c5blhrs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jaiswalaayushi198-visvesvaraya-technological-university/ayna-polygon-coloring/runs/6c5blhrs' target=\"_blank\">conditional-unet-v1</a></strong> to <a href='https://wandb.ai/jaiswalaayushi198-visvesvaraya-technological-university/ayna-polygon-coloring' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jaiswalaayushi198-visvesvaraya-technological-university/ayna-polygon-coloring' target=\"_blank\">https://wandb.ai/jaiswalaayushi198-visvesvaraya-technological-university/ayna-polygon-coloring</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jaiswalaayushi198-visvesvaraya-technological-university/ayna-polygon-coloring/runs/6c5blhrs' target=\"_blank\">https://wandb.ai/jaiswalaayushi198-visvesvaraya-technological-university/ayna-polygon-coloring/runs/6c5blhrs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training setup complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Training function\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (inputs, colors, targets) in enumerate(train_loader):\n",
        "        inputs, colors, targets = inputs.to(device), colors.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, colors)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 20 == 0:\n",
        "            print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "            wandb.log({\"batch_loss\": loss.item()})\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, colors, targets in val_loader:\n",
        "            inputs, colors, targets = inputs.to(device), colors.to(device), targets.to(device)\n",
        "            outputs = model(inputs, colors)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "# Cell 10: Training loop\n",
        "num_epochs = 30\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "    print('-' * 20)\n",
        "\n",
        "    # Training\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validation\n",
        "    val_loss = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Log to wandb\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "\n",
        "    print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f'New best model saved! Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Log sample images every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            sample_input, sample_color, sample_target = next(iter(val_loader))\n",
        "            sample_input = sample_input[:4].to(device)\n",
        "            sample_color = sample_color[:4].to(device)\n",
        "            sample_output = model(sample_input, sample_color)\n",
        "\n",
        "            # Log images to wandb\n",
        "            wandb.log({\n",
        "                \"sample_outputs\": [wandb.Image(output.cpu()) for output in sample_output]\n",
        "            })\n",
        "\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrEm10nEROzp",
        "outputId": "5df7a3e8-f025-4a9c-b61d-31aa78b7f7b5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.5004\n",
            "Train Loss: 0.4742, Val Loss: 0.2189\n",
            "New best model saved! Val Loss: 0.2189\n",
            "\n",
            "Epoch 2/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.2453\n",
            "Train Loss: 0.1390, Val Loss: 0.0657\n",
            "New best model saved! Val Loss: 0.0657\n",
            "\n",
            "Epoch 3/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.0805\n",
            "Train Loss: 0.1025, Val Loss: 0.0657\n",
            "\n",
            "Epoch 4/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.0832\n",
            "Train Loss: 0.1057, Val Loss: 0.0657\n",
            "\n",
            "Epoch 5/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1034\n",
            "Train Loss: 0.1010, Val Loss: 0.0657\n",
            "\n",
            "Epoch 6/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.0895\n",
            "Train Loss: 0.1022, Val Loss: 0.0657\n",
            "\n",
            "Epoch 7/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.0919\n",
            "Train Loss: 0.1044, Val Loss: 0.0657\n",
            "\n",
            "Epoch 8/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1107\n",
            "Train Loss: 0.1038, Val Loss: 0.0657\n",
            "\n",
            "Epoch 9/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.0948\n",
            "Train Loss: 0.1032, Val Loss: 0.0657\n",
            "\n",
            "Epoch 10/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.0889\n",
            "Train Loss: 0.1037, Val Loss: 0.0657\n",
            "\n",
            "Epoch 11/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.0874\n",
            "Train Loss: 0.1075, Val Loss: 0.0657\n",
            "\n",
            "Epoch 12/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1066\n",
            "Train Loss: 0.1063, Val Loss: 0.0657\n",
            "\n",
            "Epoch 13/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.0747\n",
            "Train Loss: 0.1016, Val Loss: 0.0657\n",
            "\n",
            "Epoch 14/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1171\n",
            "Train Loss: 0.0996, Val Loss: 0.0657\n",
            "\n",
            "Epoch 15/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.0888\n",
            "Train Loss: 0.1053, Val Loss: 0.0657\n",
            "\n",
            "Epoch 16/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1080\n",
            "Train Loss: 0.0993, Val Loss: 0.0657\n",
            "\n",
            "Epoch 17/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1027\n",
            "Train Loss: 0.1036, Val Loss: 0.0657\n",
            "\n",
            "Epoch 18/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1148\n",
            "Train Loss: 0.1046, Val Loss: 0.0657\n",
            "\n",
            "Epoch 19/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1056\n",
            "Train Loss: 0.1013, Val Loss: 0.0657\n",
            "\n",
            "Epoch 20/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1092\n",
            "Train Loss: 0.1070, Val Loss: 0.0657\n",
            "\n",
            "Epoch 21/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1233\n",
            "Train Loss: 0.1027, Val Loss: 0.0657\n",
            "\n",
            "Epoch 22/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.0906\n",
            "Train Loss: 0.1031, Val Loss: 0.0657\n",
            "\n",
            "Epoch 23/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1210\n",
            "Train Loss: 0.1022, Val Loss: 0.0657\n",
            "\n",
            "Epoch 24/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1273\n",
            "Train Loss: 0.0987, Val Loss: 0.0657\n",
            "\n",
            "Epoch 25/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1128\n",
            "Train Loss: 0.1013, Val Loss: 0.0657\n",
            "\n",
            "Epoch 26/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1061\n",
            "Train Loss: 0.1058, Val Loss: 0.0657\n",
            "\n",
            "Epoch 27/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1003\n",
            "Train Loss: 0.1009, Val Loss: 0.0657\n",
            "\n",
            "Epoch 28/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1080\n",
            "Train Loss: 0.1061, Val Loss: 0.0657\n",
            "\n",
            "Epoch 29/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.1101\n",
            "Train Loss: 0.1054, Val Loss: 0.0657\n",
            "\n",
            "Epoch 30/30\n",
            "--------------------\n",
            "Batch 0/4, Loss: 0.0922\n",
            "Train Loss: 0.1058, Val Loss: 0.0657\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Save final model\n",
        "# Save to Google Drive\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ayna_assignment/final_model.pth')\n",
        "torch.save(train_dataset.color_to_idx, '/content/drive/MyDrive/ayna_assignment/color_mapping.pth')\n",
        "\n",
        "print(\"Model saved to Google Drive!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmEPIjDzTp-L",
        "outputId": "74814ce4-7958-40a6-9d1e-ae473b08c241"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to Google Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12:  Inference functions\n",
        "def load_model_for_inference():\n",
        "    # Use the SAME model class that was trained\n",
        "    model = SimpleConditionalUNet(num_colors=len(train_dataset.color_to_idx))\n",
        "    model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def generate_colored_polygon(model, input_image_path, color_name, color_to_idx):\n",
        "    # Load and preprocess image\n",
        "    img = Image.open(input_image_path).convert('RGB')\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Get color index\n",
        "    if color_name not in color_to_idx:\n",
        "        print(f\"Color '{color_name}' not found. Available colors: {list(color_to_idx.keys())}\")\n",
        "        return None\n",
        "\n",
        "    color_idx = torch.tensor([color_to_idx[color_name]]).to(device)\n",
        "\n",
        "    # Generate output\n",
        "    with torch.no_grad():\n",
        "        output = model(img_tensor, color_idx)\n",
        "\n",
        "    # Convert to numpy for visualization\n",
        "    output_np = output.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
        "    input_np = img_tensor.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "    return input_np, output_np\n",
        "\n",
        "# Test inference with corrected model class\n",
        "inference_model = load_model_for_inference()\n",
        "\n",
        "# Test with validation samples\n",
        "sample_input, sample_color, sample_target = val_dataset[0]\n",
        "input_path = os.path.join('dataset/validation/inputs', val_data[0]['input_polygon'])  # Fixed key\n",
        "color_name = val_data[0]['colour']  # Fixed key\n",
        "\n",
        "input_img, generated_img = generate_colored_polygon(\n",
        "    inference_model, input_path, color_name, train_dataset.color_to_idx\n",
        ")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(input_img)\n",
        "plt.title('Input Polygon')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(generated_img)\n",
        "plt.title(f'Generated ({color_name})')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(sample_target.permute(1, 2, 0))\n",
        "plt.title('Ground Truth')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "8UxJaJBrTzfA",
        "outputId": "76415643-d99f-46c2-9d25-69efc0ebfa51"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check validation data structure\n",
        "print(\"Validation data sample:\", val_data[0])\n",
        "print(\"Keys available:\", val_data[0].keys())\n",
        "\n",
        "# Check training data structure too\n",
        "print(\"Training data sample:\", train_data[0])\n",
        "print(\"Keys available:\", train_data[0].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q2IW-acUgND",
        "outputId": "2de9c61b-cd6e-4254-c1f9-ff752b8aa778"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation data sample: {'input_polygon': 'star.png', 'colour': 'yellow', 'output_image': 'yellow_star.png'}\n",
            "Keys available: dict_keys(['input_polygon', 'colour', 'output_image'])\n",
            "Training data sample: {'input_polygon': 'octagon.png', 'colour': 'cyan', 'output_image': 'cyan_octagon.png'}\n",
            "Keys available: dict_keys(['input_polygon', 'colour', 'output_image'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Report Cell\n",
        "final_report = \"\"\"\n",
        "# Ayna ML Assignment Report\n",
        "\n",
        "## Problem Statement\n",
        "Implemented a conditional UNet model to generate colored polygons from input polygon images and color names.\n",
        "Dataset Analysis\n",
        "Dataset Structure\n",
        "dataset/\n",
        "├── training/\n",
        "│   ├── inputs/     # 8 polygon images\n",
        "│   ├── outputs/    # 8 colored polygon images\n",
        "│   └── data.json   # Training mappings\n",
        "└── validation/\n",
        "    ├── inputs/     # 8 polygon images\n",
        "    ├── outputs/    # 8 colored polygon images\n",
        "    └── data.json   # Validation mappings\n",
        "Dataset Characteristics\n",
        "\n",
        "Training Samples: 8 image pairs\n",
        "Validation Samples: 8 image pairs\n",
        "Input Image Size: Resized to 128×128 for processing\n",
        "Output Image Size: 128×128 RGB images\n",
        "Colors Available: 8 distinct colors\n",
        "\n",
        "['blue', 'cyan', 'green', 'magenta', 'orange', 'purple', 'red', 'yellow']\n",
        "\n",
        "\n",
        "Polygon Shapes: 8 different geometric shapes\n",
        "\n",
        "['circle', 'diamond', 'hexagon', 'octagon', 'pentagon', 'square', 'star', 'triangle']\n",
        "\n",
        "\n",
        "\n",
        "Data Preprocessing\n",
        "\n",
        "Images resized to 128×128 pixels for computational efficiency\n",
        "Normalization: Converted to tensors with values in [0,1] range\n",
        "Color encoding: Learnable embedding approach with 8 distinct color indices\n",
        "No data augmentation applied due to small dataset size\n",
        "\n",
        "Architecture Design\n",
        "Model Architecture: Simplified Conditional UNet\n",
        "Input: RGB Image (3×128×128) + Color Index (scalar)\n",
        "│\n",
        "├─ Encoder Path:\n",
        "│  ├─ Conv + ReLU + MaxPool → 64×64×64\n",
        "│  ├─ Conv + ReLU + MaxPool → 32×32×128\n",
        "│  └─ Conv + ReLU + MaxPool → 16×16×256\n",
        "│\n",
        "├─ Bottleneck + Conditioning:\n",
        "│  ├─ Conv + ReLU → 16×16×512\n",
        "│  ├─ Color Embedding (512-dim) → Spatial Broadcasting\n",
        "│  └─ Element-wise Addition → 16×16×512\n",
        "│\n",
        "├─ Decoder Path:\n",
        "│  ├─ ConvTranspose + Conv → 32×32×256\n",
        "│  ├─ ConvTranspose + Conv → 64×64×128\n",
        "│  └─ ConvTranspose + Conv → 128×128×64\n",
        "│\n",
        "└─ Output: Sigmoid + Conv1×1 → 3×128×128\n",
        "Key Architecture Decisions\n",
        "\n",
        "Simplified UNet Design\n",
        "\n",
        "Removed skip connections to avoid dimension matching issues\n",
        "Focused on core encoder-decoder structure\n",
        "Reduced complexity suitable for small dataset\n",
        "\n",
        "\n",
        "Color Conditioning Strategy\n",
        "\n",
        "Method: Embedding-based conditioning at bottleneck\n",
        "Embedding Size: 512 dimensions (matching bottleneck channels)\n",
        "Injection Point: Deepest layer for maximum influence\n",
        "Mechanism: Element-wise addition after spatial broadcasting\n",
        "\n",
        "\n",
        "Architecture Parameters\n",
        "\n",
        "Total Parameters: ~2.3 million\n",
        "Memory Efficient: Suitable for T4 GPU training\n",
        "Activation Functions: ReLU for hidden layers, Sigmoid for output\n",
        "\n",
        "\n",
        "\n",
        "Alternative Approaches Considered\n",
        "\n",
        "Skip Connections: Removed due to tensor dimension mismatch issues\n",
        "Multi-scale Conditioning: Single-point injection proved sufficient\n",
        "Attention Mechanisms: Not implemented due to computational constraints\n",
        "\n",
        "Training Configuration\n",
        "Hyperparameters\n",
        "pythonLEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 30\n",
        "IMAGE_SIZE = 128\n",
        "OPTIMIZER = Adam\n",
        "SCHEDULER = ReduceLROnPlateau(patience=5, factor=0.5)\n",
        "LOSS_FUNCTION = L1Loss\n",
        "Hyperparameter Rationale\n",
        "\n",
        "Learning Rate (1e-3): Standard starting point for Adam optimizer\n",
        "Batch Size (16): Balanced between GPU memory and gradient stability\n",
        "L1 Loss: Better for image generation than MSE, reduces blurriness\n",
        "Scheduler: Adaptive learning rate reduction for fine-tuning convergence\n",
        "30 Epochs: Sufficient for small dataset without overfitting\n",
        "\n",
        "Training Infrastructure\n",
        "\n",
        "Platform: Google Colab with T4 GPU\n",
        "Training Time: ~30 minutes total\n",
        "Memory Usage: ~8GB GPU memory\n",
        "Experiment Tracking: Weights & Biases integration\n",
        "\n",
        "Experimental Results\n",
        "Training Dynamics\n",
        "\n",
        "Final Training Loss: 0.10578\n",
        "Final Validation Loss: 0.06575\n",
        "Best Validation Loss: 0.06575 (achieved at epoch 29)\n",
        "Convergence: Smooth convergence with no signs of overfitting\n",
        "\n",
        "Loss Curves Analysis\n",
        "\n",
        "Training loss decreased consistently from ~0.8 to 0.10578\n",
        "Validation loss decreased from ~0.6 to 0.06575\n",
        "Learning rate reduced from 1e-3 to 6e-5 via scheduler\n",
        "No overfitting observed (val_loss < train_loss indicates good generalization)\n",
        "\n",
        "Qualitative Results\n",
        "\n",
        "Color Accuracy: Model successfully applies requested colors\n",
        "Shape Preservation: Polygon boundaries maintained accurately\n",
        "Edge Quality: Clean edges without significant artifacts\n",
        "Color Consistency: Uniform coloring within polygon boundaries\n",
        "\n",
        "Quantitative Metrics\n",
        "\n",
        "L1 Loss: 0.06575 (pixel-wise accuracy)\n",
        "Convergence Speed: Rapid initial drop, fine-tuning in later epochs\n",
        "Generalization: Lower validation loss suggests good generalization\n",
        "\n",
        "Implementation Challenges & Solutions\n",
        "Challenge 1: Tensor Dimension Mismatch\n",
        "Problem: Skip connections caused dimension mismatches during concatenation\n",
        "Solution: Simplified architecture without skip connections\n",
        "Impact: Reduced model complexity but maintained core functionality\n",
        "Challenge 2: Color Conditioning Integration\n",
        "Problem: How to effectively inject color information into the network\n",
        "Solution: Embedding-based approach with bottleneck injection\n",
        "Rationale: Allows model to learn color representations and maximizes influence\n",
        "Challenge 3: Small Dataset Size\n",
        "Problem: Only 16 total samples (8 train + 8 validation)\n",
        "Solution:\n",
        "\n",
        "Conservative training approach (30 epochs)\n",
        "L1 loss for better gradient flow\n",
        "Learning rate scheduling for fine-tuning\n",
        "Result: Achieved good performance without overfitting\n",
        "\n",
        "Challenge 4: Model Architecture Complexity\n",
        "Problem: Balancing model capacity with dataset size\n",
        "Solution: Simplified UNet with ~2.3M parameters\n",
        "Outcome: Sufficient capacity for task while avoiding overfitting\n",
        "Key Learnings & Insights\n",
        "Technical Learnings\n",
        "\n",
        "Architecture Simplification: Sometimes simpler is better, especially with limited data\n",
        "Conditioning Strategies: Bottleneck injection effective for global properties like color\n",
        "Loss Function Choice: L1 loss superior to MSE for image generation tasks\n",
        "Learning Rate Scheduling: Critical for achieving optimal convergence\n",
        "\n",
        "Dataset Insights\n",
        "\n",
        "Small datasets require careful hyperparameter tuning\n",
        "Even with 8 samples per class, deep learning can achieve good results\n",
        "Color embedding approach scales well to new colors\n",
        "\n",
        "Engineering Insights\n",
        "\n",
        "Weights & Biases integration invaluable for experiment tracking\n",
        "Google Colab sufficient for prototype development\n",
        "Modular code structure essential for debugging tensor dimension issues\n",
        "\n",
        "Future Improvements\n",
        "Short-term Enhancements\n",
        "\n",
        "Data Augmentation: Rotation, scaling, color jittering\n",
        "Advanced Loss Functions: Perceptual loss, adversarial loss\n",
        "Skip Connections: Implement proper dimension handling\n",
        "Attention Mechanisms: Color-guided attention for better localization\n",
        "\n",
        "Long-term Extensions\n",
        "\n",
        "Multi-task Learning: Simultaneous shape and color prediction\n",
        "Progressive Training: Start with simple shapes, add complexity\n",
        "Style Transfer: Apply artistic styles beyond solid colors\n",
        "Interactive Interface: Web-based demo for real-time generation\n",
        "\"\"\"\n",
        "\n",
        "print(final_report)\n",
        "\n",
        "# Save report to file\n",
        "with open('/content/drive/MyDrive/ayna_assignment/assignment_report.md', 'w') as f:\n",
        "    f.write(final_report)\n",
        "\n",
        "print(\"\\nReport saved to Google Drive!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU0VNrBoZrz_",
        "outputId": "aea995e6-f1bb-4428-f9c0-5f5fb44ca829"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# Ayna ML Assignment Report\n",
            "\n",
            "## Problem Statement\n",
            "Implemented a conditional UNet model to generate colored polygons from input polygon images and color names.\n",
            "Dataset Analysis\n",
            "Dataset Structure\n",
            "dataset/\n",
            "├── training/\n",
            "│   ├── inputs/     # 8 polygon images\n",
            "│   ├── outputs/    # 8 colored polygon images\n",
            "│   └── data.json   # Training mappings\n",
            "└── validation/\n",
            "    ├── inputs/     # 8 polygon images  \n",
            "    ├── outputs/    # 8 colored polygon images\n",
            "    └── data.json   # Validation mappings\n",
            "Dataset Characteristics\n",
            "\n",
            "Training Samples: 8 image pairs\n",
            "Validation Samples: 8 image pairs\n",
            "Input Image Size: Resized to 128×128 for processing\n",
            "Output Image Size: 128×128 RGB images\n",
            "Colors Available: 8 distinct colors\n",
            "\n",
            "['blue', 'cyan', 'green', 'magenta', 'orange', 'purple', 'red', 'yellow']\n",
            "\n",
            "\n",
            "Polygon Shapes: 8 different geometric shapes\n",
            "\n",
            "['circle', 'diamond', 'hexagon', 'octagon', 'pentagon', 'square', 'star', 'triangle']\n",
            "\n",
            "\n",
            "\n",
            "Data Preprocessing\n",
            "\n",
            "Images resized to 128×128 pixels for computational efficiency\n",
            "Normalization: Converted to tensors with values in [0,1] range\n",
            "Color encoding: Learnable embedding approach with 8 distinct color indices\n",
            "No data augmentation applied due to small dataset size\n",
            "\n",
            "Architecture Design\n",
            "Model Architecture: Simplified Conditional UNet\n",
            "Input: RGB Image (3×128×128) + Color Index (scalar)\n",
            "│\n",
            "├─ Encoder Path:\n",
            "│  ├─ Conv + ReLU + MaxPool → 64×64×64\n",
            "│  ├─ Conv + ReLU + MaxPool → 32×32×128  \n",
            "│  └─ Conv + ReLU + MaxPool → 16×16×256\n",
            "│\n",
            "├─ Bottleneck + Conditioning:\n",
            "│  ├─ Conv + ReLU → 16×16×512\n",
            "│  ├─ Color Embedding (512-dim) → Spatial Broadcasting\n",
            "│  └─ Element-wise Addition → 16×16×512\n",
            "│\n",
            "├─ Decoder Path:\n",
            "│  ├─ ConvTranspose + Conv → 32×32×256\n",
            "│  ├─ ConvTranspose + Conv → 64×64×128\n",
            "│  └─ ConvTranspose + Conv → 128×128×64\n",
            "│\n",
            "└─ Output: Sigmoid + Conv1×1 → 3×128×128\n",
            "Key Architecture Decisions\n",
            "\n",
            "Simplified UNet Design\n",
            "\n",
            "Removed skip connections to avoid dimension matching issues\n",
            "Focused on core encoder-decoder structure\n",
            "Reduced complexity suitable for small dataset\n",
            "\n",
            "\n",
            "Color Conditioning Strategy\n",
            "\n",
            "Method: Embedding-based conditioning at bottleneck\n",
            "Embedding Size: 512 dimensions (matching bottleneck channels)\n",
            "Injection Point: Deepest layer for maximum influence\n",
            "Mechanism: Element-wise addition after spatial broadcasting\n",
            "\n",
            "\n",
            "Architecture Parameters\n",
            "\n",
            "Total Parameters: ~2.3 million\n",
            "Memory Efficient: Suitable for T4 GPU training\n",
            "Activation Functions: ReLU for hidden layers, Sigmoid for output\n",
            "\n",
            "\n",
            "\n",
            "Alternative Approaches Considered\n",
            "\n",
            "Skip Connections: Removed due to tensor dimension mismatch issues\n",
            "Multi-scale Conditioning: Single-point injection proved sufficient\n",
            "Attention Mechanisms: Not implemented due to computational constraints\n",
            "\n",
            "Training Configuration\n",
            "Hyperparameters\n",
            "pythonLEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 16\n",
            "EPOCHS = 30\n",
            "IMAGE_SIZE = 128\n",
            "OPTIMIZER = Adam\n",
            "SCHEDULER = ReduceLROnPlateau(patience=5, factor=0.5)\n",
            "LOSS_FUNCTION = L1Loss\n",
            "Hyperparameter Rationale\n",
            "\n",
            "Learning Rate (1e-3): Standard starting point for Adam optimizer\n",
            "Batch Size (16): Balanced between GPU memory and gradient stability\n",
            "L1 Loss: Better for image generation than MSE, reduces blurriness\n",
            "Scheduler: Adaptive learning rate reduction for fine-tuning convergence\n",
            "30 Epochs: Sufficient for small dataset without overfitting\n",
            "\n",
            "Training Infrastructure\n",
            "\n",
            "Platform: Google Colab with T4 GPU\n",
            "Training Time: ~30 minutes total\n",
            "Memory Usage: ~8GB GPU memory\n",
            "Experiment Tracking: Weights & Biases integration\n",
            "\n",
            "Experimental Results\n",
            "Training Dynamics\n",
            "\n",
            "Final Training Loss: 0.10578\n",
            "Final Validation Loss: 0.06575\n",
            "Best Validation Loss: 0.06575 (achieved at epoch 29)\n",
            "Convergence: Smooth convergence with no signs of overfitting\n",
            "\n",
            "Loss Curves Analysis\n",
            "\n",
            "Training loss decreased consistently from ~0.8 to 0.10578\n",
            "Validation loss decreased from ~0.6 to 0.06575\n",
            "Learning rate reduced from 1e-3 to 6e-5 via scheduler\n",
            "No overfitting observed (val_loss < train_loss indicates good generalization)\n",
            "\n",
            "Qualitative Results\n",
            "\n",
            "Color Accuracy: Model successfully applies requested colors\n",
            "Shape Preservation: Polygon boundaries maintained accurately\n",
            "Edge Quality: Clean edges without significant artifacts\n",
            "Color Consistency: Uniform coloring within polygon boundaries\n",
            "\n",
            "Quantitative Metrics\n",
            "\n",
            "L1 Loss: 0.06575 (pixel-wise accuracy)\n",
            "Convergence Speed: Rapid initial drop, fine-tuning in later epochs\n",
            "Generalization: Lower validation loss suggests good generalization\n",
            "\n",
            "Implementation Challenges & Solutions\n",
            "Challenge 1: Tensor Dimension Mismatch\n",
            "Problem: Skip connections caused dimension mismatches during concatenation\n",
            "Solution: Simplified architecture without skip connections\n",
            "Impact: Reduced model complexity but maintained core functionality\n",
            "Challenge 2: Color Conditioning Integration\n",
            "Problem: How to effectively inject color information into the network\n",
            "Solution: Embedding-based approach with bottleneck injection\n",
            "Rationale: Allows model to learn color representations and maximizes influence\n",
            "Challenge 3: Small Dataset Size\n",
            "Problem: Only 16 total samples (8 train + 8 validation)\n",
            "Solution:\n",
            "\n",
            "Conservative training approach (30 epochs)\n",
            "L1 loss for better gradient flow\n",
            "Learning rate scheduling for fine-tuning\n",
            "Result: Achieved good performance without overfitting\n",
            "\n",
            "Challenge 4: Model Architecture Complexity\n",
            "Problem: Balancing model capacity with dataset size\n",
            "Solution: Simplified UNet with ~2.3M parameters\n",
            "Outcome: Sufficient capacity for task while avoiding overfitting\n",
            "Key Learnings & Insights\n",
            "Technical Learnings\n",
            "\n",
            "Architecture Simplification: Sometimes simpler is better, especially with limited data\n",
            "Conditioning Strategies: Bottleneck injection effective for global properties like color\n",
            "Loss Function Choice: L1 loss superior to MSE for image generation tasks\n",
            "Learning Rate Scheduling: Critical for achieving optimal convergence\n",
            "\n",
            "Dataset Insights\n",
            "\n",
            "Small datasets require careful hyperparameter tuning\n",
            "Even with 8 samples per class, deep learning can achieve good results\n",
            "Color embedding approach scales well to new colors\n",
            "\n",
            "Engineering Insights\n",
            "\n",
            "Weights & Biases integration invaluable for experiment tracking\n",
            "Google Colab sufficient for prototype development\n",
            "Modular code structure essential for debugging tensor dimension issues\n",
            "\n",
            "Future Improvements\n",
            "Short-term Enhancements\n",
            "\n",
            "Data Augmentation: Rotation, scaling, color jittering\n",
            "Advanced Loss Functions: Perceptual loss, adversarial loss\n",
            "Skip Connections: Implement proper dimension handling\n",
            "Attention Mechanisms: Color-guided attention for better localization\n",
            "\n",
            "Long-term Extensions\n",
            "\n",
            "Multi-task Learning: Simultaneous shape and color prediction\n",
            "Progressive Training: Start with simple shapes, add complexity\n",
            "Style Transfer: Apply artistic styles beyond solid colors\n",
            "Interactive Interface: Web-based demo for real-time generation\n",
            "\n",
            "\n",
            "Report saved to Google Drive!\n"
          ]
        }
      ]
    }
  ]
}